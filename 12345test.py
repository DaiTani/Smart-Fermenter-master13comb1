import sys
import time
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torch.backends.cudnn as cudnn
import os
import argparse
import numpy as np
from datasetC import *
import pdb
import warnings
from model import *
import random
import utils
import math
import pandas as pd
import matplotlib.pyplot as plt
import json
from Ga import ParameterOptimizer
from filelock import Timeout, FileLock
from collections import deque
import pymysql
from sqlalchemy import create_engine
import pymysql.cursors
import warnings

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore")

# æ³¨æ„åŠ›å¢å¼ºçš„LSTMæ ¡æ­£æ¨¡å—
class AttentionLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.attention = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )
        self.fc = nn.Linear(hidden_dim, 1)
      
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)
        context = torch.sum(attn_weights * lstm_out, dim=1)
        return self.fc(context)

# å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“
class RLAgent:
    def __init__(self, state_dim):
        self.gamma = 0.99
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.memory = deque(maxlen=2000)
      
        self.policy_net = AttentionLSTM(state_dim, 32).cuda()
        self.target_net = AttentionLSTM(state_dim, 32).cuda()
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.001)
  
    def select_action(self, state):
        if np.random.rand() <= self.epsilon:
            return np.random.uniform(-0.1, 0.1)
        else:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0).cuda()
                return self.policy_net(state_tensor).cpu().item()
  
    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))
  
    def replay(self, batch_size):
        if len(self.memory) < batch_size:
            return
      
        minibatch = random.sample(self.memory, batch_size)
        states, actions, rewards, next_states, dones = zip(*minibatch)
      
        states = torch.FloatTensor(np.array(states)).unsqueeze(1).cuda()
        actions = torch.FloatTensor(actions).cuda()
        rewards = torch.FloatTensor(rewards).cuda()
        if next_states[0] is not None:
            next_states = torch.FloatTensor(np.array(next_states)).unsqueeze(1).cuda()
        else:
            next_states = None
        dones = torch.FloatTensor(dones).cuda()
      
        current_q = self.policy_net(states).squeeze()
      
        # å¤„ç†ç»ˆæ­¢çŠ¶æ€
        with torch.no_grad():
            if next_states is None:
                next_q = torch.zeros_like(current_q)
            else:
                next_q = self.target_net(next_states).squeeze()
      
        target_q = rewards + (1 - dones) * self.gamma * next_q
      
        loss = nn.MSELoss()(current_q, target_q)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
      
        if self.epsilon > self.epsilon_min:
            self.epsilon *= 0.995
  
    def update_target(self):
        self.target_net.load_state_dict(self.policy_net.state_dict())

# æ•°æ®åº“æ“ä½œç±»
class MySQLDatabase:
    def __init__(self, host, user, password, database, port=3306):
        self.host = host
        self.user = user
        self.password = password
        self.database = database
        self.port = port
        self.engine = create_engine(f"mysql+pymysql://{user}:{password}@{host}:{port}/{database}")
      
    def query(self, sql):
        """æ‰§è¡ŒSQLæŸ¥è¯¢å¹¶è¿”å›ç»“æœ"""
        connection = pymysql.connect(
            host=self.host,
            user=self.user,
            password=self.password,
            database=self.database,
            port=self.port,
            cursorclass=pymysql.cursors.DictCursor
        )
        try:
            with connection.cursor() as cursor:
                cursor.execute(sql)
                result = cursor.fetchall()
            return result
        finally:
            connection.close()
  
    def insert(self, table, data):
        """æ’å…¥æ•°æ®åˆ°è¡¨ä¸­"""
        connection = pymysql.connect(
            host=self.host,
            user=self.user,
            password=self.password,
            database=self.database,
            port=self.port
        )
        try:
            with connection.cursor() as cursor:
                # æ„å»ºæ’å…¥è¯­å¥
                columns = ', '.join(data.keys())
                placeholders = ', '.join(['%s'] * len(data))
                sql = f"INSERT INTO {table} ({columns}) VALUES ({placeholders})"
                cursor.execute(sql, list(data.values()))
            connection.commit()
        finally:
            connection.close()
  
    def get_last_record(self, table):
        """è·å–è¡¨ä¸­æœ€åä¸€æ¡è®°å½•"""
        result = self.query(f"SELECT * FROM {table} ORDER BY id DESC LIMIT 1")
        if result:
            return result[0]
        return None
  
    def update_last_record(self, table, data):
        """æ›´æ–°è¡¨ä¸­æœ€åä¸€æ¡è®°å½•"""
        last_id = self.get_last_record(table)['id']
        connection = pymysql.connect(
            host=self.host,
            user=self.user,
            password=self.password,
            database=self.database,
            port=self.port
        )
        try:
            with connection.cursor() as cursor:
                # æ„å»ºæ›´æ–°è¯­å¥
                columns = ', '.join([f"{k} = %s" for k in data.keys()])
                sql = f"UPDATE {table} SET {columns} WHERE id = %s"
                values = list(data.values()) + [last_id]
                cursor.execute(sql, values)
            connection.commit()
        finally:
            connection.close()

    def get_row_count(self, table):
        """è·å–è¡¨ä¸­çš„è¡Œæ•°"""
        result = self.query(f"SELECT COUNT(*) AS count FROM {table}")
        if result:
            return result[0]['count']
        return 0

# æ ¡æ­£ç³»ç»Ÿ
class ModelCorrector:
    """æ¨¡å‹æ ¡æ­£ç³»ç»Ÿ"""
    def __init__(self, base_model, state_dim, y_mean, y_std):
        self.base_model = base_model
        self.agent = RLAgent(state_dim=3*11 + 1)  # æœ€å3æ­¥ç‰¹å¾+é¢„æµ‹å€¼
        self.y_mean = y_mean
        self.y_std = y_std
  
    def prepare_state(self, features, prediction):
        """æ„å»ºå¼ºåŒ–å­¦ä¹ çŠ¶æ€å‘é‡"""
        recent_features = features[-3:].flatten()  # ä»…ä½¿ç”¨æœ€å3ä¸ªæ—¶é—´æ­¥
        return np.concatenate([recent_features, [prediction]])
  
    def correct_prediction(self, features):
        """ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ ¡æ­£é¢„æµ‹"""
        with torch.no_grad():
            h = self.base_model.init_hidden(1)
            input_tensor = torch.FloatTensor(features).unsqueeze(0).cuda()
            base_pred, _ = self.base_model(input_tensor, h)
            base_pred = base_pred[-1][-1].item()
      
        state = self.prepare_state(features, base_pred)
        correction = self.agent.select_action(state)
        return base_pred + correction

    def train_corrector(self, dataset, epochs=10, batch_size=32):
        """ä½¿ç”¨å†å²æ•°æ®è®­ç»ƒæ ¡æ­£å™¨"""
        # é¢„è®¡ç®—åŸºç¡€é¢„æµ‹
        self.base_model.eval()
        base_preds = []
        for features, true_val, _ in dataset:
            with torch.no_grad():
                h = self.base_model.init_hidden(1)
                input_tensor = torch.FloatTensor(features).unsqueeze(0).cuda()
                pred, _ = self.base_model(input_tensor, h)
                preds_denorm1 = pred * self.y_std + self.y_mean
                initial_od6001 = preds_denorm1[-1][-1]
                pred_value = initial_od6001.item()

                # æ£€æŸ¥é¢„æµ‹å€¼æ˜¯å¦æœ‰æ•ˆ
                if np.isnan(pred_value) or np.isinf(pred_value):
                    print(f"è­¦å‘Š: æ— æ•ˆçš„åŸºç¡€é¢„æµ‹å€¼: {pred_value}")
                    pred_value = 0.0  # è®¾ä¸ºé»˜è®¤å€¼
              
                base_preds.append(pred_value)
      
        # å¼ºåŒ–å­¦ä¹ è®­ç»ƒ
        self.agent.policy_net.train()
        for epoch in range(epochs):
            states, actions, rewards = [], [], []
            valid_samples = 0
          
            for idx, (features, true_val, _) in enumerate(dataset):
                # è·³è¿‡æ— æ•ˆæ•°æ®ç‚¹
                if idx >= len(base_preds):
                    continue
                  
                if np.isnan(true_val) or np.isinf(true_val):
                    print(f"è·³è¿‡æ— æ•ˆçœŸå®å€¼: {true_val}")
                    continue
                  
                state = self.prepare_state(features, base_preds[idx])
                state = state.astype(np.float32)  # ç¡®ä¿æ•°æ®ç±»å‹
              
                # æ£€æŸ¥çŠ¶æ€æ˜¯å¦æœ‰æ•ˆ
                if np.isnan(state).any() or np.isinf(state).any():
                    print(f"è·³è¿‡æ— æ•ˆçŠ¶æ€: {state}")
                    continue
                                      
                action = self.agent.select_action(state)
                corrected_pred = base_preds[idx] + action
                 # è®¡ç®—å¥–åŠ±ï¼šè´Ÿç»å¯¹è¯¯å·®ï¼Œæ·»åŠ å°å€¼é˜²æ­¢NaN
                error = abs(corrected_pred - true_val)
                if np.isnan(error) or error == 0:
                    error = 1e-6  # é˜²æ­¢é™¤ä»¥é›¶æˆ–NaN
                  
                reward = -error
                rewards.append(reward)
              
                # å­˜å‚¨ç»éªŒï¼ˆå•æ­¥ç»ˆæ­¢ï¼‰
                self.agent.remember(state, action, reward, None, True)
              
                # è®°å½•æ‰¹æ¬¡æ•°æ®
                states.append(state)
                actions.append(action)
                valid_samples += 1
          
            if valid_samples == 0:
                print(f"Epoch {epoch+1} | æ— æœ‰æ•ˆæ ·æœ¬")
                continue
              
            # æ‰¹é‡ç»éªŒå›æ”¾
            self.agent.replay(batch_size)
          
            # è®¡ç®—å¹³å‡å¥–åŠ±
            avg_reward = np.mean(rewards)
            print(f"Epoch {epoch+1} | Avg Reward: {avg_reward:.4f} | æœ‰æ•ˆæ ·æœ¬: {valid_samples}")
          
            # æ›´æ–°ç›®æ ‡ç½‘ç»œ
            if epoch % 5 == 0:
                self.agent.update_target()
        self.agent.policy_net.eval()

# æ•°æ®å‡†å¤‡å‡½æ•°
def load_correction_data(db, data_table, pred_table, seq_length=20):
    """ä»MySQLæ•°æ®åº“åŠ è½½æ ¡æ­£è®­ç»ƒæ•°æ®"""
    # è¯»å–åŸå§‹æ•°æ®
    data_query = f"SELECT * FROM {data_table}"
    pred_query = f"SELECT * FROM {pred_table}"
  
    data_df = pd.DataFrame(db.query(data_query))
    pred_df = pd.DataFrame(db.query(pred_query))
  
    # åŠ¨æ€è·å–æ ‡ç­¾
    all_columns = data_df.columns.tolist()
    x_columns = [col for col in all_columns if col not in ["Timestamp", "od_600"]]
  
    # æŒ‰æ—¶é—´æˆ³åˆå¹¶
    merged_df = pd.merge(data_df, pred_df, on="Timestamp", suffixes=('_true', '_pred'))
  
    feature_columns = [f"{col}_true" for col in x_columns]
    features = merged_df[feature_columns].values 
    true_od = merged_df['od_600_true'].values
    pred_od = merged_df['od_600_pred'].values
  
    # æ„å»ºæ•°æ®é›†
    dataset = []
    valid_count = 0
    for i in range(seq_length, len(features)):
        # å½“å‰åºåˆ—
        seq_features = features[i-seq_length:i]
        true_value = true_od[i]
      
        # è·³è¿‡æ— æ•ˆæ•°æ®ç‚¹
        if np.isnan(seq_features).any() or np.isnan(true_value):
            print(f"è·³è¿‡æ— æ•ˆæ•°æ®ç‚¹ (ç´¢å¼• {i})")
            continue
          
        dataset.append((seq_features, true_value, None))
        valid_count += 1
  
    print(f"åŠ è½½æ ¡æ­£æ•°æ®: å…± {len(features)} ä¸ªç‚¹, æœ‰æ•ˆ {valid_count} ä¸ª")
    return dataset

def main():
    parser = argparse.ArgumentParser(description="PyTorch CIFAR Training")
    parser.add_argument("--batch_size", default=256, type=int, help="test batchsize")
    parser.add_argument("--hidden_dim", default=16, type=int)
    parser.add_argument("--num_layers", default=2, type=int)
    parser.add_argument("--seed", default=123)
    parser.add_argument("--gpuid", default=0, type=int)
    parser.add_argument("--weights", type=str, default="")
    parser.add_argument("--dataset", type=str, default="")
    parser.add_argument("--model", default="lstm", type=str)
    parser.add_argument("--pred_steps", default=1, type=int, help="Number of prediction steps")
    parser.add_argument("--train_corrector", action="store_true", help="æ˜¯å¦è®­ç»ƒå¼ºåŒ–å­¦ä¹ æ ¡æ­£å™¨")
    parser.add_argument("--corrector_epochs", type=int, default=10, help="æ ¡æ­£å™¨è®­ç»ƒè½®æ•°")
  
    # æ•°æ®åº“é…ç½®å‚æ•°
    parser.add_argument("--db_host", default="localhost", type=str, help="MySQL host")
    parser.add_argument("--db_user", default="root", type=str, help="MySQL user")
    parser.add_argument("--db_password", default="password", type=str, help="MySQL password")
    parser.add_argument("--db_name", default="fermentation", type=str, help="MySQL database name")
    parser.add_argument("--db_port", default=3306, type=int, help="MySQL port")
    parser.add_argument("--data_table", default="data_table", type=str, help="åŸå§‹æ•°æ®è¡¨å")
    parser.add_argument("--result_table", default="data_table", type=str, help="ç»“æœè¡¨å")
  
    args = parser.parse_args()
  
    # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
    db = MySQLDatabase(
        host=args.db_host,
        user=args.db_user,
        password=args.db_password,
        database=args.db_name,
        port=args.db_port
    )
  
    print("ğŸ‘‰ æ•°æ®åº“è¿æ¥åˆå§‹åŒ–å®Œæˆï¼Œå¼€å§‹åˆ›å»ºç»“æœè¡¨...")
  
    # åˆ›å»ºç»“æœè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    create_table_sql = f"""
    CREATE TABLE IF NOT EXISTS {args.result_table} (
        id INT AUTO_INCREMENT PRIMARY KEY,
        Timestamp FLOAT,
        dm_air FLOAT,
        m_ls_opt_do FLOAT,
        m_ph FLOAT,
        m_stirrer FLOAT,
        m_temp FLOAT,
        dm_o2 FLOAT,
        dm_spump1 FLOAT,
        dm_spump2 FLOAT,
        dm_spump3 FLOAT,
        dm_spump4 FLOAT,
        induction FLOAT,
        od_600 FLOAT,
        RMSE FLOAT,
        REFY FLOAT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
    """
    db.query(create_table_sql)
  
    print("ğŸ‘‰ ç»“æœè¡¨åˆ›å»ºå®Œæˆï¼Œå¼€å§‹åŠ è½½å½’ä¸€åŒ–å‚æ•°...")
  
    torch.cuda.set_device(args.gpuid)
    random.seed(args.seed)
    torch.manual_seed(args.seed)
    torch.cuda.manual_seed_all(args.seed)
  
    # è·å–æ•°æ®åˆ—å
    data_df = pd.DataFrame(db.query(f"SELECT * FROM {args.data_table} LIMIT 1"))
    all_columns = data_df.columns.tolist()
    x_columns = [col for col in all_columns if col not in ["Timestamp", "od_600"]]
    n_features = len(x_columns)
  
    # åŠ è½½å½’ä¸€åŒ–å‚æ•°
    norm_file_path = r"C:\Users\YZDR\OneDrive\æ¡Œé¢\Smart-Fermenter-master13comb1\Data5\norm_file.json"
    with open(norm_file_path, 'r') as f:
        norm_data = json.load(f)
    y_mean = norm_data['y_mean']
    y_std = norm_data['y_std']
  
    # è®¾ç½®æ¨¡å‹
    if args.model == "lstm":
        model = LSTMPredictor(
            input_dim=n_features,
            hidden_dim=args.hidden_dim,
            output_dim=1,
            n_layers=args.num_layers,
        ).cuda()
    elif args.model == "rnn":
        model = RNNpredictor(
            input_dim=n_features,
            hidden_dim=args.hidden_dim,
            output_dim=1,
            n_layers=args.num_layers,
        ).cuda()
  
    print("ğŸ‘‰ æ¨¡å‹è®¾ç½®å®Œæˆï¼Œå¼€å§‹åŠ è½½æƒé‡...")
  
    # åŠ è½½æ¨¡å‹æƒé‡
    weights = (
        os.path.join("model_weights", "od600_prediction", args.model, "weights_best.tar")
        if args.weights == ""
        else args.weights
    )
    # model = utils.load_weights(model, weights)
    model.eval()
    mse = nn.MSELoss()
  
    print("ğŸ‘‰ æƒé‡åŠ è½½å®Œæˆï¼Œå¼€å§‹ä¸»ç›‘æ§å¾ªç¯...")
    # åˆå§‹åŒ–æ ¡æ­£å™¨
    corrector = ModelCorrector(
        base_model=model, 
        state_dim=3 * n_features + 1,
        y_mean=y_mean, 
        y_std=y_std
    )
  
    # è®­ç»ƒæ ¡æ­£å™¨ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.train_corrector:
        print("\n===== è®­ç»ƒå¼ºåŒ–å­¦ä¹ æ ¡æ­£å™¨ =====")
        correction_data = load_correction_data(
            db=db,
            data_table=args.data_table,
            pred_table=args.result_table,
            seq_length=20
        )
        corrector.train_corrector(correction_data, epochs=args.corrector_epochs)
        torch.save(corrector.agent.policy_net.state_dict(), "rl_corrector.pth")
        print("æ ¡æ­£å™¨æ¨¡å‹å·²ä¿å­˜è‡³ rl_corrector.pth")
  
    # åŠ è½½è®­ç»ƒå¥½çš„æ ¡æ­£å™¨
    if os.path.exists("rl_corrector.pth"):
        corrector.agent.policy_net.load_state_dict(torch.load("rl_corrector.pth"))
        corrector.agent.policy_net.eval()
        print("å·²åŠ è½½é¢„è®­ç»ƒæ ¡æ­£å™¨")

    # Predict with overlapped sequences
    def test(epoch, model, testloader, corrector=None):
        model.eval()
        loss = 0
        err = 0
        iter = 0
        h = model.init_hidden(args.batch_size)
        preds = np.zeros(len(test_dataset) + test_dataset.ws - 1)
        labels = np.zeros(len(test_dataset) + test_dataset.ws - 1)
        n_overlap = np.zeros(len(test_dataset) + test_dataset.ws - 1)
        N = 10
      
        with torch.no_grad():
            for batch_idx, (input, label) in enumerate(testloader):
                iter += 1
                batch_size = input.size(0)
                h = model.init_hidden(batch_size)
                h = tuple([e.data.cuda() for e in h])
                input, label = input.cuda(), label.cuda()
              
                # ä½¿ç”¨æ ¡æ­£é¢„æµ‹å™¨
                if corrector:
                    if input.dim() == 2:
                        input = input.unsqueeze(0)
                  
                    y = []
                    for i in range(input.size(0)):
                        seq = input[i]
                        seq_np = seq.cpu().numpy()
                        corrected_pred = corrector.correct_prediction(seq_np)
                        y.append(corrected_pred)
                    y = np.array(y)
                else:
                    output, h = model(input.float().cuda(), h)
                    y = output.view(-1).cpu().numpy()
              
                y_padded = np.pad(y, (N // 2, N - 1 - N // 2), mode="edge")
                y_smooth = np.convolve(y_padded, np.ones((N,)) / N, mode="valid")
              
                preds[batch_idx: (batch_idx + test_dataset.ws)] += y_smooth
                labels[batch_idx: (batch_idx + test_dataset.ws)] += label.view(-1).cpu().numpy()
                n_overlap[batch_idx: (batch_idx + test_dataset.ws)] += 1.0
              
                # è®¡ç®—æŸå¤±
                y_tensor = torch.tensor([y]).float().cuda()
                loss += nn.MSELoss()(y_tensor, label.float().cuda())
                err += torch.sqrt(nn.MSELoss()(y_tensor, label.float().cuda())).item()
        loss = loss / len(test_dataset)
        err = err / len(test_dataset)
        preds /= n_overlap
        labels /= n_overlap
        return err, preds, labels

    def _normalize_individual(optimized_params):
        """å½’ä¸€åŒ– individual æ•°æ®"""
        return (optimized_params - x_mean) / x_std
  
    # åˆå§‹åŒ–è¡Œæ•°è®¡æ•°å™¨
    INITIAL_ROW_COUNT = db.get_row_count(args.data_table)
  
    # è®­ç»ƒé˜ˆå€¼è®¾ç½®
    TRAIN_THRESHOLD = 2
  
    # ä¸»ç›‘æ§å¾ªç¯
    last_timestamp = None

    while True:
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ•°æ®
            last_record = db.get_last_record(args.data_table)
            if last_record:
                current_timestamp = last_record['Timestamp']
                print(f"å½“å‰æ—¶é—´æˆ³: {current_timestamp}")
              
                # å¦‚æœæ²¡æœ‰æ–°æ•°æ®ï¼Œç­‰å¾…å¹¶ç»§ç»­å¾ªç¯
                if current_timestamp == last_timestamp:
                    print("æœªæ£€æµ‹åˆ°æ–°æ•°æ®ï¼Œç­‰å¾…ä¸­...")
                    time.sleep(1)
                    continue
              
                print(f"\næ£€æµ‹åˆ°æ–°æ•°æ®: {current_timestamp}")
                last_timestamp = current_timestamp
              
                # åŠ è½½æ•°æ®
                # ä¿®å¤ï¼šé¿å…åœ¨Yä¸Šè°ƒç”¨cumulative2snapshot
                test_dataset = FermentationData(
                    work_dir=args.dataset, 
                    train_mode=False, 
                    y_var=["od_600"],
                    y_mean=y_mean,
                    y_std=y_std,
                    skip_y_cumulative=True  # å…³é”®ä¿®å¤ï¼šè·³è¿‡Yçš„ç´¯ç§¯å¤„ç†
                )
                print(f"åŠ è½½çš„æ•°æ®é›†å¤§å°: {len(test_dataset)}") 
              
                # è·å– X çš„å½’ä¸€åŒ–å‚æ•°
                x_mean, x_std = utils.get_norm_param(X=test_dataset.X_norm, x_cols=test_dataset.x_var)
              
                # è®¾ç½®DataLoader
                test_loader = torch.utils.data.DataLoader(
                    dataset=test_dataset, batch_size=1, num_workers=0, shuffle=False
                )
              
                # å‚æ•°ä¼˜åŒ–ç›¸å…³åˆå§‹åŒ–
                param_names = test_dataset.x_var
                initial_state = test_dataset.X[-1][-1].copy() * x_std + x_mean
                initial_params = initial_state[:n_features].copy()
                print(f"Initial params: {initial_params}")
              
                epsilon = 1e-6
              
                # è®¾ç½®å‚æ•°è¾¹ç•Œ
                param_bounds = []
                for i in range(n_features):
                    feature_name = x_columns[i]
                    if feature_name == "m_ph":
                        param_bounds.append((3.0, 8.0))
                    elif feature_name == "induction":
                        param_bounds.append((0.1, 0.9))
                    else:
                        low = max(epsilon, initial_params[i] - initial_params[i]*0.1)
                        high = initial_params[i] + initial_params[i]*0.1
                        param_bounds.append((low, high))
              
                # ç¬¬ä¸€è½®æµ‹è¯•
                print("\nInitial Testing")
                err, preds, labels = test(0, model, test_loader, corrector)
                preds_denorm = preds * y_std + y_mean
                initial_od600 = preds_denorm[-1]
                print(f"Initial predict OD600: {initial_od600}")
              
                # å‚æ•°ä¼˜åŒ–
                optimizer = ParameterOptimizer(param_names, param_bounds, model, 
                                            test_dataset, x_mean, x_std, y_mean, y_std)
                optimized_params = test_dataset.X[-1][-1] * x_std + x_mean
                best_od600 = initial_od600
                best_params = optimized_params.copy()
              
                # æ‰§è¡Œä¼˜åŒ–
                optimized_params = optimizer.optimize(initial_params, best_od600)
              
                # æ›´æ–°æ•°æ®é›†å‚æ•°
                optimized_params_norm = _normalize_individual(optimized_params)
                test_dataset.X[-1][-1] = optimized_params_norm
              
                # é‡æ–°æµ‹è¯•
                err, preds, labels = test(0, model, test_loader, corrector)
                preds_denorm = preds * y_std + y_mean
                current_od600 = preds_denorm[-1]
              
                # æ›´æ–°æœ€ä½³ç»“æœ
                if current_od600 > best_od600:
                    best_od600 = current_od600
                    best_params = optimized_params
              
                # è·å–æ—¶é—´æˆ³
                timestamp = current_timestamp
              
                # é€†å½’ä¸€åŒ–é¢„æµ‹ç»“æœå’Œæ ‡ç­¾
                preds_denorm = preds * y_std + y_mean
                labels_denorm = labels * y_std + y_mean
                preds_denorm = preds_denorm.reshape(-1, 1)
                labels_denorm = labels_denorm.reshape(-1, 1)
                preds_denorm = preds_denorm[50:]
                labels_denorm = labels_denorm[50:]
                mse = np.square(np.subtract(preds_denorm, labels_denorm)).mean()
                rmse = math.sqrt(mse)
              
                # Relative Error on Final Yield
                refy = abs(preds_denorm[-1] - labels_denorm[-1]) / labels_denorm[-1] * 100
              
                # æ›´æ–°æœ€åä¸€æ¡è®°å½•
                update_data = {
                    "od_600_pred": current_od600,
                    "RMSE": rmse,
                    "REFY": refy
                }
              
                db.update_last_record(args.data_table, update_data)
                print(f"ç»“æœå·²æ›´æ–°è‡³æ•°æ®åº“è¡¨ {args.data_table} çš„æœ€åä¸€æ¡è®°å½•")
              
                print("\nOptimization Results:")
                print(f"Initial predict OD600: {initial_od600}, Initial parameters: {initial_params}")
                print(f"Best OD600: {best_od600}, Best parameters: {best_params}")
              
                # ä¿å­˜åˆ° results.npz
                np.savez(
                    weights.split("/weights")[0] + "/results.npz",
                    preds=preds_denorm,
                    labels=labels_denorm,
                    rmse=rmse,
                    refy=refy,
                )
                print("Saved: ", weights.split("/weights")[0] + "/results.npz")
                print("\nRMSE Error OD600: ", rmse)
                print("\nREFY: %.2f%%" % (refy), "[absolute error: %.2f]" % (abs(preds_denorm[-1] - labels_denorm[-1])))
              
                # ç»˜åˆ¶æ›²çº¿
                utils.plot_od600_curve(
                    preds_denorm, labels_denorm, weights[:-17], rmse, refy
                )
              
                # æ£€æŸ¥æ˜¯å¦éœ€è¦è®­ç»ƒæ ¡æ­£å™¨
                current_row_count = db.get_row_count(args.data_table)
                new_rows_added = current_row_count - INITIAL_ROW_COUNT
                if new_rows_added >= TRAIN_THRESHOLD:
                    print(f"\n===== æ–°å¢{new_rows_added}è¡Œæ•°æ®ï¼Œå¼€å§‹è®­ç»ƒæ ¡æ­£å™¨ =====")
                  
                    # åŠ è½½æ ¡æ­£æ•°æ®
                    correction_data = load_correction_data(
                        db=db,
                        data_table=args.data_table,
                        pred_table=args.data_table,
                        seq_length=20
                    )
                  
                    # è®­ç»ƒæ ¡æ­£å™¨
                    corrector.train_corrector(correction_data, epochs=args.corrector_epochs)
                  
                    # ä¿å­˜å¹¶é‡æ–°åŠ è½½æ ¡æ­£å™¨æ¨¡å‹
                    torch.save(corrector.agent.policy_net.state_dict(), "rl_corrector.pth")
                    corrector.agent.policy_net.load_state_dict(torch.load("rl_corrector.pth"))
                    corrector.agent.policy_net.eval()
                    print(f"æ ¡æ­£å™¨æ¨¡å‹å·²æ›´æ–°ï¼ˆåŸºäº{current_row_count}è¡Œæ•°æ®ï¼‰")
                  
                    # æ›´æ–°åˆå§‹è¡Œæ•°åŸºå‡†
                    INITIAL_ROW_COUNT = current_row_count
      
            # ç­‰å¾…30ç§’å†æ¬¡æ£€æŸ¥
            time.sleep(1)
      
        except KeyboardInterrupt:
            print("\nç›‘æ§å·²åœæ­¢")
            break
        except Exception as e:
            print(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
            time.sleep(1)  # å‡ºé”™åç­‰å¾…30ç§’é‡è¯•

if __name__ == '__main__':
    main()